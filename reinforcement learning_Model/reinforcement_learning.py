# -*- coding: utf-8 -*-
"""reinforcement learning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HhBwrF_TFRTKqxlbFZ2HP-NGtlAkF08s
"""

import pandas as pd

df = pd.read_csv('/content/CDM_DATA_YOTA.csv')
df.head()

import pandas as pd

df = pd.read_csv('/content/CDM_DATA_YOTA.csv', low_memory=False)
df.head()

"""## Inspect Data and Identify Objects

### Subtask:
Inspect the loaded data to understand its structure, identify relevant columns for 'object1' and 'object2', and extract features pertinent to satellite maneuver optimization and coupling risks.

**Reasoning**:
To understand the DataFrame's structure, data types, and descriptive statistics, I will display the column names, print a concise summary using `df.info()`, and show descriptive statistics using `df.describe()`.
"""

print("DataFrame Columns:\n", df.columns.tolist())
print("\nDataFrame Info:")
df.info()
print("\nDescriptive Statistics:")
df.describe()

"""### Identified Columns for Object1

The following columns appear to pertain to 'object1':

*   `object1_object_designator`
*   `object1_catalog_name`
*   `object1_object_name`
*   `object1_international_designator`
*   `object1_object_type`
*   `object1_maneuverable`
*   `object1_orbit_center`
*   `object1_ref_frame`
*   `object1_cov_type`
*   `comment_object1_od_parameters`
*   `object1_crdot_t`
*   `object1_crdot_n`
*   `object1_crdot_rdot`
*   `object1_ctdot_r`
*   `object1_ctdot_t`
*   `object1_ctdot_n`
*   `object1_ctdot_rdot`
*   `object1_ctdot_tdot`
*   `object1_cndot_r`
*   `object1_cndot_t`
*   `object1_cndot_n`
*   `object1_cndot_rdot`
*   `object1_cndot_tdot`
*   `object1_cndot_ndot`

### Identified Columns for Object2

The following columns appear to pertain to 'object2':

*   `object2_object_designator`
*   `object2_catalog_name`
*   `object2_object_name`
*   `object2_international_designator`
*   `object2_object_type`
*   `object2_ephemeris_name`
*   `object2_covariance_method`
*   `object2_maneuverable`
*   `object2_orbit_center`
*   `object2_ref_frame`
*   `object2_cov_type`
*   `object2_x`
*   `object2_y`
*   `object2_z`
*   `object2_x_dot`
*   `object2_y_dot`
*   `object2_z_dot`
*   `comment_object2_covmatrix`
*   `object2_cr_r`
*   `object2_ct_r`
*   `object2_ct_t`
*   `object2_cn_r`
*   `object2_cn_t`
*   `object2_cn_n`
*   `object2_crdot_r`
*   `object2_crdot_t`
*   `object2_crdot_n`
*   `object2_crdot_rdot`
*   `object2_ctdot_r`
*   `object2_ctdot_t`
*   `object2_ctdot_n`
*   `object2_ctdot_rdot`
*   `object2_ctdot_tdot`
*   `object2_cndot_r`
*   `object2_cndot_t`
*   `object2_cndot_n`
*   `object2_cndot_rdot`
*   `object2_cndot_tdot`
*   `object2_cndot_ndot`

### Collision Risk and Maneuver-Relevant Columns

The following columns are relevant for assessing collision risk or performing maneuvers:

*   `miss_distance`
*   `relative_speed`
*   `relative_position_r`
*   `relative_position_t`
*   `relative_position_n`
*   `relative_velocity_r`
*   `relative_velocity_t`
*   `relative_velocity_n`
*   `collision_probability`
*   `collision_max_probability`
*   `object1_maneuverable`
*   `object2_maneuverable`
*   `screen_volume_radius`
*   `tca` (Time of Closest Approach)
*   `start_screen_period`
*   `stop_screen_period`

## Define RL Environment Components

### Subtask:
Based on the identified objects and data, define the key components of the reinforcement learning environment, including the state space (e.g., relative positions, velocities), action space (e.g., thrust commands, maneuver types), reward function (e.g., collision avoidance, fuel efficiency), and transition dynamics for 'object1' and 'object2'.

## Define RL Environment Components

### State Space
The state space will comprise the following columns from the DataFrame, which represent the relative orbital characteristics and collision risk parameters between 'object1' and 'object2':
- `miss_distance`: Closest approach distance.
- `relative_speed`: Relative speed between the objects.
- `relative_position_r`, `relative_position_t`, `relative_position_n`: Relative position components in the radial, tangential, and normal directions.
- `relative_velocity_r`, `relative_velocity_t`, `relative_velocity_n`: Relative velocity components in the radial, tangential, and normal directions.
- `collision_probability`: Probability of collision.
- `collision_max_probability`: Maximum collision probability.

Additionally, to inform the agent about its own characteristics, the state could also include:
- `object1_maneuverable`: A boolean flag indicating if object1 can maneuver.
- `object1_object_type`: Type of object1.

### Action Space
The action space for 'object1' will be discrete, representing various thrust commands to alter its trajectory. Assuming 'object1' is the maneuverable satellite, the actions could be defined as changes in velocity components (`delta-v`) along the radial, tangential, and normal axes. For simplicity, we can define a set of discrete thrusts:
- No maneuver (0, 0, 0)
- Small positive/negative thrust in R, T, N directions (e.g., +delta_v_r, -delta_v_r, +delta_v_t, -delta_v_t, +delta_v_n, -delta_v_n).

If `object1_maneuverable` is 'N' (No), the agent's action space would be restricted to only 'No maneuver'. If it's 'Y' (Yes), the full set of maneuvers would be available.

### Reward Function
The reward function should guide the agent towards collision avoidance while considering maneuver costs:
- **Positive Reward**: A large positive reward for successfully increasing the `miss_distance` above a safety threshold (e.g., 100 km) and/or reducing `collision_probability` to near zero.
- **Negative Reward (Penalty)**:
    - A large negative reward (collision penalty) if `miss_distance` falls below a critical threshold (e.g., 1 km) or `collision_probability` exceeds a high threshold.
    - A small negative reward (maneuver cost) for each maneuver executed, proportional to the thrust magnitude, to encourage fuel efficiency.
    - A continuous negative reward inversely proportional to `miss_distance` and/or directly proportional to `collision_probability` to encourage maintaining a safe separation.

### Transition Dynamics
Given a current state (relative positions and velocities) and an action (a chosen delta-v maneuver for 'object1'), the environment's transition dynamics would describe how the state evolves to the next time step. This would involve:
1.  **Applying the maneuver**: If an action is chosen, the `relative_velocity_r/t/n` components of 'object1' would be updated based on the `delta-v` applied.
2.  **Orbital Propagation**: The new velocities and positions would then be propagated forward in time (e.g., to the next TCA or a fixed time step) to calculate the new `miss_distance`, `relative_speed`, `relative_position_r/t/n`, `relative_velocity_r/t/n`, and consequently, the new `collision_probability` and `collision_max_probability`.
3.  **External Factors**: 'object2' is assumed to follow its predicted trajectory (passive object), and 'object1' is the active agent. The environment would essentially simulate the two-body orbital mechanics or use simplified models to update the relative state vectors after a maneuver.

This would form the basis for how an agent interacts with the environment and learns to optimize its maneuvers.

## Implement RL Environment

### Subtask:
Implement a basic reinforcement learning environment that simulates the satellite maneuver problem, incorporating the dynamics of 'object1' and 'object2' and the defined state, action, and reward structures. This might involve using a library like Gymnasium or a custom implementation.

**Reasoning**:
To implement the RL environment, I will start by defining the `SatelliteManeuverEnv` class, inheriting from `gymnasium.Env`. I will initialize the environment by defining the observation and action spaces based on the provided data and task description, and setting up the internal state.
"""

import gymnasium as gym
import numpy as np

class SatelliteManeuverEnv(gym.Env):
    metadata = {'render_modes': ['human'], 'render_fps': 30}

    def __init__(self, df):
        super().__init__()
        self.df = df
        self.current_step = 0
        self.num_conjunctions = len(df)

        # Define observation space based on relevant columns
        # Features: miss_distance, relative_speed, relative_position_r/t/n, relative_velocity_r/t/n, collision_probability, collision_max_probability
        # We need to determine the min/max values for these features from the dataframe
        low_bounds = df[['miss_distance', 'relative_speed',
                         'relative_position_r', 'relative_position_t', 'relative_position_n',
                         'relative_velocity_r', 'relative_velocity_t', 'relative_velocity_n',
                         'collision_probability', 'collision_max_probability']].min().values
        high_bounds = df[['miss_distance', 'relative_speed',
                          'relative_position_r', 'relative_position_t', 'relative_position_n',
                          'relative_velocity_r', 'relative_velocity_t', 'relative_velocity_n',
                          'collision_probability', 'collision_max_probability']].max().values

        # Add a small buffer to bounds to prevent issues with exact min/max values during normalization or simulation
        low_bounds[0:2] = 0 # Miss distance and relative speed cannot be negative
        low_bounds[8:] = 0 # Probabilities cannot be negative

        # Ensure all values are finite before creating the Box space
        low_bounds = np.nan_to_num(low_bounds, nan=-np.inf)
        high_bounds = np.nan_to_num(high_bounds, nan=np.inf)

        self.observation_space = gym.spaces.Box(low=low_bounds, high=high_bounds, dtype=np.float32)

        # Define action space:
        # 0: no maneuver
        # 1-2: delta-v in R (positive/negative)
        # 3-4: delta-v in T (positive/negative)
        # 5-6: delta-v in N (positive/negative)
        # The magnitude of delta-v can be fixed for simplicity in this initial implementation
        self.delta_v_magnitude = 0.1 # Example magnitude in km/s (adjust as needed)
        self.action_space = gym.spaces.Discrete(7) # 7 discrete actions

    def _get_obs(self):
        current_event = self.df.iloc[self.current_step]
        obs = current_event[['miss_distance', 'relative_speed',
                             'relative_position_r', 'relative_position_t', 'relative_position_n',
                             'relative_velocity_r', 'relative_velocity_t', 'relative_velocity_n',
                             'collision_probability', 'collision_max_probability']].values.astype(np.float32)
        return obs

    def _get_info(self):
        current_event = self.df.iloc[self.current_step]
        return {'object1_maneuverable': current_event['object1_maneuverable']}

    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        # For simplicity, start from the first conjunction event
        self.current_step = 0
        observation = self._get_obs()
        info = self._get_info()
        return observation, info

    def step(self, action):
        current_event = self.df.iloc[self.current_step]

        # Apply delta-v based on action if object1 is maneuverable
        if self._get_info()['object1_maneuverable'] == 'Y': # Assuming 'Y' for maneuverable
            # For simplicity, we directly modify relative velocity, which is not physically accurate
            # A proper simulation would update orbital elements and re-calculate relative state.
            # Here, we'll just simulate a change that would reduce relative velocity in some axis.
            # In a real scenario, applying a delta-v changes the *absolute* velocity of object1.
            # This would then affect *relative* position and velocity over time.
            # For this basic implementation, we will just use dummy changes to demonstrate the concept.

            # Create a copy of the relative velocities to simulate impact
            relative_velocity = current_event[['relative_velocity_r', 'relative_velocity_t', 'relative_velocity_n']].values.astype(np.float32)

            if action == 1: # +R direction
                relative_velocity[0] -= self.delta_v_magnitude # Reduce component in R
            elif action == 2: # -R direction
                relative_velocity[0] += self.delta_v_magnitude # Reduce component in R
            elif action == 3: # +T direction
                relative_velocity[1] -= self.delta_v_magnitude # Reduce component in T
            elif action == 4: # -T direction
                relative_velocity[1] += self.delta_v_magnitude # Reduce component in T
            elif action == 5: # +N direction
                relative_velocity[2] -= self.delta_v_magnitude # Reduce component in N
            elif action == 6: # -N direction
                relative_velocity[2] += self.delta_v_magnitude # Reduce component in N

            # Update the relative velocity values in the current event (for next step's observation)
            # In a real environment, this would be part of a propagation model.
            # For now, we'll just reflect this change in the observation for demonstration.
            # This is a highly simplified model and needs proper orbital mechanics for accuracy.
            current_event['relative_velocity_r'] = relative_velocity[0]
            current_event['relative_velocity_t'] = relative_velocity[1]
            current_event['relative_velocity_n'] = relative_velocity[2]

        # Move to the next conjunction event for the next observation (simplified propagation)
        self.current_step += 1
        terminated = False
        truncated = False
        reward = 0.0

        if self.current_step >= self.num_conjunctions: # End of data
            terminated = True
            # If episode ends without collision, provide a positive reward
            if current_event['collision_probability'] < 1e-6: # Example threshold for safe separation
                reward = 100.0
            else:
                reward = -100.0 # Punish if still high probability at the end of episode
        else:
            # Define a simple reward: negative for high collision probability, positive for low
            current_prob = current_event['collision_probability']
            if current_prob > 1e-4: # Example threshold for high risk
                reward = -10.0
            elif current_prob < 1e-6: # Example threshold for safe
                reward = 1.0
            else:
                reward = -1.0 # Neutral or slightly negative for moderate risk

            # Also penalize for maneuvering (action > 0)
            if action > 0:
                reward -= 0.1 # Small penalty for fuel consumption

        observation = self._get_obs()
        info = self._get_info()

        return observation, reward, terminated, truncated, info

# Instantiate the environment to verify it works
env = SatelliteManeuverEnv(df)
print("Environment created successfully!")
print(f"Observation Space: {env.observation_space}")
print(f"Action Space: {env.action_space}")

# Test a reset and a step
obs, info = env.reset()
print("\nInitial Observation (first conjunction event):", obs)
print("Initial Info:", info)

# Take a dummy action (e.g., no maneuver)
# For demonstration, we'll take one step and then print the new state.
# In a real RL loop, this would happen iteratively.

# Assuming object1_maneuverable is 'Y' for the first entry to test action effect
# If it's 'N', actions 1-6 won't have a direct effect on relative_velocity as implemented above
# We'll override the maneuverability for a quick test if it's 'N' in the actual data
initial_maneuverable_status = df.iloc[0]['object1_maneuverable']
if initial_maneuverable_status != 'Y':
    print("\nWarning: object1 in the first conjunction is not maneuverable. Action effects will not be visible with current simplified model.")
    print("Continuing with dummy step without visual effect for demonstration.")

action_to_take = 0 # No maneuver
print(f"\nTaking action: {action_to_take} (No maneuver)")
obs, reward, terminated, truncated, info = env.step(action_to_take)
print("Observation after one step:", obs)
print("Reward after one step:", reward)
print("Terminated after one step:", terminated)
print("Info after one step:", info)

# Take another action (e.g., delta-v in +R direction) if object1 is maneuverable in the new state
if info['object1_maneuverable'] == 'Y':
    action_to_take = 1 # Delta-v in +R
    print(f"\nTaking action: {action_to_take} (Delta-v in +R direction)")
    obs, reward, terminated, truncated, info = env.step(action_to_take)
    print("Observation after second step:", obs)
    print("Reward after second step:", reward)
    print("Terminated after second step:", terminated)
    print("Info after second step:", info)

"""**Reasoning**:
The previous code produced a `UserWarning` about precision loss during `gymnasium.spaces.Box` initialization and highlighted an issue where `object1_maneuverable` could be `NaN`, preventing correct action application. More importantly, the `step` method's logic was flawed as it advanced to the next conjunction event from the DataFrame, effectively losing the impact of the agent's action on the current conjunction. To fix this, I will modify `reset` to select a random conjunction event for an episode and `step` to update a mutable internal state (`self._current_state`) with a simplified heuristic for maneuver effects on `miss_distance` and `collision_probability` within that single conjunction, rather than simply moving to the next row of the DataFrame.


"""

import gymnasium as gym
import numpy as np
import pandas as pd

class SatelliteManeuverEnv(gym.Env):
    metadata = {'render_modes': ['human'], 'render_fps': 30}

    def __init__(self, df):
        super().__init__()
        self.df = df.copy() # Work on a copy to avoid modifying original df
        self.num_conjunctions = len(df)
        self.max_steps_per_episode = 10 # Define maximum steps to simulate a single conjunction event

        # Handle NaN in 'object1_maneuverable' by filling with 'N'
        if 'object1_maneuverable' in self.df.columns:
            self.df['object1_maneuverable'] = self.df['object1_maneuverable'].fillna('N').astype(str)
        else:
            # If column doesn't exist, assume not maneuverable for all
            self.df['object1_maneuverable'] = 'N'


        # Define observation space based on relevant columns
        feature_columns = ['miss_distance', 'relative_speed',
                         'relative_position_r', 'relative_position_t', 'relative_position_n',
                         'relative_velocity_r', 'relative_velocity_t', 'relative_velocity_n',
                         'collision_probability', 'collision_max_probability']

        low_bounds = self.df[feature_columns].min().values
        high_bounds = self.df[feature_columns].max().values

        # Ensure logical bounds for specific features
        low_bounds[0] = 0.0 # miss_distance cannot be negative
        low_bounds[1] = 0.0 # relative_speed cannot be negative
        low_bounds[8] = 0.0 # collision_probability min is 0
        low_bounds[9] = 0.0 # collision_max_probability min is 0
        high_bounds[8] = 1.0 # collision_probability max is 1
        high_bounds[9] = 1.0 # collision_max_probability max is 1


        # Ensure all values are finite before creating the Box space
        low_bounds = np.nan_to_num(low_bounds, nan=-np.inf)
        high_bounds = np.nan_to_num(high_bounds, nan=np.inf)

        self.observation_space = gym.spaces.Box(low=low_bounds, high=high_bounds, dtype=np.float32)

        # Define action space:
        # 0: no maneuver
        # 1-2: delta-v in R (positive/negative)
        # 3-4: delta-v in T (positive/negative)
        # 5-6: delta-v in N (positive/negative)
        self.delta_v_magnitude = 0.01 # Adjusted example magnitude in km/s (smaller, more realistic)
        self.action_space = gym.spaces.Discrete(7) # 7 discrete actions

        # Internal state that gets updated within an episode
        self._current_state = None
        self._current_conjunction_idx = None # Index of the conjunction from df for the current episode
        self._steps_taken_in_episode = 0

    def _get_obs(self):
        # Return a copy to prevent external modification of internal state
        return self._current_state.copy()

    def _get_info(self):
        # Ensure 'object1_maneuverable' is a string 'Y' or 'N'
        maneuverable_status = self.df.iloc[self._current_conjunction_idx]['object1_maneuverable']
        return {'object1_maneuverable': str(maneuverable_status)}

    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        # Randomly select a conjunction event from the DataFrame to start a new episode
        self._current_conjunction_idx = self.np_random.integers(self.num_conjunctions)
        self._steps_taken_in_episode = 0

        # Initialize the internal mutable state for the episode
        current_event_data = self.df.iloc[self._current_conjunction_idx]

        feature_columns = ['miss_distance', 'relative_speed',
                         'relative_position_r', 'relative_position_t', 'relative_position_n',
                         'relative_velocity_r', 'relative_velocity_t', 'relative_velocity_n',
                         'collision_probability', 'collision_max_probability']
        self._current_state = current_event_data[feature_columns].values.astype(np.float32)

        # Ensure probabilities are within [0, 1] bounds, and other physical bounds
        self._current_state[8] = np.clip(self._current_state[8], 0.0, 1.0) # collision_probability
        self._current_state[9] = np.clip(self._current_state[9], 0.0, 1.0) # collision_max_probability
        self._current_state[0] = np.maximum(self._current_state[0], 0.0) # miss_distance
        self._current_state[1] = np.maximum(self._current_state[1], 0.0) # relative_speed


        observation = self._get_obs()
        info = self._get_info()
        return observation, info

    def step(self, action):
        self._steps_taken_in_episode += 1
        terminated = False
        truncated = False
        reward = 0.0

        maneuverable = self._get_info()['object1_maneuverable'] == 'Y'

        # Apply delta-v based on action if object1 is maneuverable
        if maneuverable and action > 0:
            # A temporary copy of velocity components from the current internal state
            relative_velocity = self._current_state[5:8].copy() # Indices for r_v, t_v, n_v

            if action == 1: # +R direction (reduce current R component)
                relative_velocity[0] -= self.delta_v_magnitude
            elif action == 2: # -R direction (increase current R component)
                relative_velocity[0] += self.delta_v_magnitude
            elif action == 3: # +T direction (reduce current T component)
                relative_velocity[1] -= self.delta_v_magnitude
            elif action == 4: # -T direction (increase current T component)
                relative_velocity[1] += self.delta_v_magnitude
            elif action == 5: # +N direction (reduce current N component)
                relative_velocity[2] -= self.delta_v_magnitude
            elif action == 6: # -N direction (increase current N component)
                relative_velocity[2] += self.delta_v_magnitude

            # Update the internal state with new relative velocities
            self._current_state[5:8] = relative_velocity

            # --- Simplified Heuristic for propagation effect of maneuver ---
            # This is NOT physically accurate but demonstrates how action could change collision risk.
            # In a real system, an orbital propagator would re-calculate miss_distance and collision_probability.

            current_miss_dist = self._current_state[0]
            current_prob = self._current_state[8]

            # A very simplistic model: small delta-v directly improves miss distance and reduces probability
            # Factor could be tuned or derived from a more complex model
            miss_dist_increase_per_delta_v = 1000.0 # Example: 1000m increase per km/s delta_v
            prob_decrease_factor_per_delta_v = 0.5 # Example: reduce probability by 50% per delta_v

            # Impact of maneuver on miss_distance and collision_probability
            # The effect is proportional to delta_v_magnitude
            self._current_state[0] += miss_dist_increase_per_delta_v * self.delta_v_magnitude # Increase miss distance
            self._current_state[8] *= (1.0 - prob_decrease_factor_per_delta_v * self.delta_v_magnitude) # Reduce collision probability
            self._current_state[9] *= (1.0 - prob_decrease_factor_per_delta_v * self.delta_v_magnitude) # Reduce max collision probability

            # Clip probabilities to valid range [0, 1]
            self._current_state[8] = np.clip(self._current_state[8], 0.0, 1.0)
            self._current_state[9] = np.clip(self._current_state[9], 0.0, 1.0)

            # Penalize for maneuvering (action > 0)
            reward -= 0.1 # Small penalty for fuel consumption

        # Define reward based on the *updated* internal state
        current_prob = self._current_state[8]
        current_miss_dist = self._current_state[0]

        safe_miss_distance_threshold = 100.0 # km
        critical_miss_distance_threshold = 5.0 # km
        safe_prob_threshold = 1e-7 # Very low probability
        high_prob_threshold = 1e-4 # High risk probability

        if current_prob < safe_prob_threshold and current_miss_dist > safe_miss_distance_threshold:
            reward += 10.0 # Significant positive reward for very safe state
            terminated = True
        elif current_prob > high_prob_threshold or current_miss_dist < critical_miss_distance_threshold:
            reward -= 50.0 # Large negative penalty for high risk/collision
            terminated = True
        else:
            # Continuous reward based on inverse probability and miss distance
            # Higher miss_distance is good, lower probability is good
            # Ensure no division by zero for current_prob
            reward += (1.0 / (current_prob + 1e-12)) * 1e-3 + (current_miss_dist / 1000.0) # Scale appropriately

        # Episode termination conditions
        if self._steps_taken_in_episode >= self.max_steps_per_episode:
            truncated = True # Episode ends due to time limit

        # Ensure observation values are within defined bounds after all updates
        self._current_state[0] = np.clip(self._current_state[0], self.observation_space.low[0], self.observation_space.high[0])
        self._current_state[1] = np.clip(self._current_state[1], self.observation_space.low[1], self.observation_space.high[1])
        # Clip relative positions and velocities too, though they are usually unbounded in Box space
        for i in range(2, 8):
            self._current_state[i] = np.clip(self._current_state[i], self.observation_space.low[i], self.observation_space.high[i])
        self._current_state[8] = np.clip(self._current_state[8], self.observation_space.low[8], self.observation_space.high[8])
        self._current_state[9] = np.clip(self._current_state[9], self.observation_space.low[9], self.observation_space.high[9])


        observation = self._get_obs()
        info = self._get_info()

        return observation, reward, terminated, truncated, info

# Instantiate the environment to verify it works
env = SatelliteManeuverEnv(df)
print("Environment created successfully!")
print(f"Observation Space: {env.observation_space}")
print(f"Action Space: {env.action_space}")

# Test a reset and a step
obs, info = env.reset()
print("\nInitial Observation (first conjunction event):", obs)
print("Initial Info:", info)
print("Maneverability of current conjunction: ", info['object1_maneuverable'])

# Take a dummy action (e.g., no maneuver)
action_to_take = 0 # No maneuver
print(f"\nTaking action: {action_to_take} (No maneuver)")
obs, reward, terminated, truncated, info = env.step(action_to_take)
print("Observation after one step:", obs)
print("Reward after one step:", reward)
print("Terminated after one step:", terminated)
print("Truncated after one step:", truncated)
print("Info after one step:", info)

# Take another action (e.g., delta-v in +R direction) if object1 is maneuverable in the current state
if info['object1_maneuverable'] == 'Y':
    action_to_take = 1 # Delta-v in +R
    print(f"\nTaking action: {action_to_take} (Delta-v in +R direction)")
    obs, reward, terminated, truncated, info = env.step(action_to_take)
    print("Observation after second step:", obs)
    print("Reward after second step:", reward)
    print("Terminated after second step:", terminated)
    print("Truncated after second step:", truncated)
    print("Info after second step:", info)
else:
    print("\nSkipping maneuver action test as object1 is not maneuverable in this episode.")

"""**Reasoning**:
To fix the `UserWarning` about precision loss, I will explicitly cast the calculated `low_bounds` and `high_bounds` to `np.float32` before passing them to `gym.spaces.Box`. This ensures that Gymnasium receives bounds in the expected dtype and avoids the implicit conversion warning. Additionally, I will correct the `np.nan_to_num` arguments for `posinf` and `neginf` to use `np.finfo(np.float32).max` and `np.finfo(np.float32).min` respectively, ensuring that any infinities in the bounds are converted to the maximum/minimum finite values representable by `float32`, which is more robust when dealing with potential `inf` values from min/max calculations on large datasets.


"""

import gymnasium as gym
import numpy as np
import pandas as pd

class SatelliteManeuverEnv(gym.Env):
    metadata = {'render_modes': ['human'], 'render_fps': 30}

    def __init__(self, df):
        super().__init__()
        self.df = df.copy() # Work on a copy to avoid modifying original df
        self.num_conjunctions = len(df)
        self.max_steps_per_episode = 10 # Define maximum steps to simulate a single conjunction event

        # Handle NaN in 'object1_maneuverable' by filling with 'N'
        if 'object1_maneuverable' in self.df.columns:
            self.df['object1_maneuverable'] = self.df['object1_maneuverable'].fillna('N').astype(str)
        else:
            # If column doesn't exist, assume not maneuverable for all
            self.df['object1_maneuverable'] = 'N'


        # Define observation space based on relevant columns
        feature_columns = ['miss_distance', 'relative_speed',
                         'relative_position_r', 'relative_position_t', 'relative_position_n',
                         'relative_velocity_r', 'relative_velocity_t', 'relative_velocity_n',
                         'collision_probability', 'collision_max_probability']

        # Calculate bounds and explicitly cast to float32 to avoid UserWarning
        low_bounds = self.df[feature_columns].min().values.astype(np.float32)
        high_bounds = self.df[feature_columns].max().values.astype(np.float32)

        # Ensure logical bounds for specific features
        low_bounds[0] = 0.0 # miss_distance cannot be negative
        low_bounds[1] = 0.0 # relative_speed cannot be negative
        low_bounds[8] = 0.0 # collision_probability min is 0
        low_bounds[9] = 0.0 # collision_max_probability min is 0
        high_bounds[8] = 1.0 # collision_probability max is 1
        high_bounds[9] = 1.0 # collision_max_probability max is 1


        # Ensure all values are finite before creating the Box space, converting inf to float32 max/min
        low_bounds = np.nan_to_num(low_bounds, nan=-np.inf, posinf=np.finfo(np.float32).max, neginf=np.finfo(np.float32).min)
        high_bounds = np.nan_to_num(high_bounds, nan=np.inf, posinf=np.finfo(np.float32).max, neginf=np.finfo(np.float32).min)

        self.observation_space = gym.spaces.Box(low=low_bounds, high=high_bounds, dtype=np.float32)

        # Define action space:
        # 0: no maneuver
        # 1-2: delta-v in R (positive/negative)
        # 3-4: delta-v in T (positive/negative)
        # 5-6: delta-v in N (positive/negative)
        self.delta_v_magnitude = 0.01 # Adjusted example magnitude in km/s (smaller, more realistic)
        self.action_space = gym.spaces.Discrete(7) # 7 discrete actions

        # Internal state that gets updated within an episode
        self._current_state = None
        self._current_conjunction_idx = None # Index of the conjunction from df for the current episode
        self._steps_taken_in_episode = 0

    def _get_obs(self):
        # Return a copy to prevent external modification of internal state
        return self._current_state.copy()

    def _get_info(self):
        # Ensure 'object1_maneuverable' is a string 'Y' or 'N'
        maneuverable_status = self.df.iloc[self._current_conjunction_idx]['object1_maneuverable']
        return {'object1_maneuverable': str(maneuverable_status)}

    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        # Randomly select a conjunction event from the DataFrame to start a new episode
        self._current_conjunction_idx = self.np_random.integers(self.num_conjunctions)
        self._steps_taken_in_episode = 0

        # Initialize the internal mutable state for the episode
        current_event_data = self.df.iloc[self._current_conjunction_idx]

        feature_columns = ['miss_distance', 'relative_speed',
                         'relative_position_r', 'relative_position_t', 'relative_position_n',
                         'relative_velocity_r', 'relative_velocity_t', 'relative_velocity_n',
                         'collision_probability', 'collision_max_probability']
        self._current_state = current_event_data[feature_columns].values.astype(np.float32)

        # Ensure probabilities are within [0, 1] bounds, and other physical bounds
        self._current_state[8] = np.clip(self._current_state[8], 0.0, 1.0) # collision_probability
        self._current_state[9] = np.clip(self._current_state[9], 0.0, 1.0) # collision_max_probability
        self._current_state[0] = np.maximum(self._current_state[0], 0.0) # miss_distance
        self._current_state[1] = np.maximum(self._current_state[1], 0.0) # relative_speed


        observation = self._get_obs()
        info = self._get_info()
        return observation, info

    def step(self, action):
        self._steps_taken_in_episode += 1
        terminated = False
        truncated = False
        reward = 0.0

        maneuverable = self._get_info()['object1_maneuverable'] == 'Y'

        # Apply delta-v based on action if object1 is maneuverable
        if maneuverable and action > 0:
            # A temporary copy of velocity components from the current internal state
            relative_velocity = self._current_state[5:8].copy() # Indices for r_v, t_v, n_v

            if action == 1: # +R direction (reduce current R component)
                relative_velocity[0] -= self.delta_v_magnitude
            elif action == 2: # -R direction (increase current R component)
                relative_velocity[0] += self.delta_v_magnitude
            elif action == 3: # +T direction (reduce current T component)
                relative_velocity[1] -= self.delta_v_magnitude
            elif action == 4: # -T direction (increase current T component)
                relative_velocity[1] += self.delta_v_magnitude
            elif action == 5: # +N direction (reduce current N component)
                relative_velocity[2] -= self.delta_v_magnitude
            elif action == 6: # -N direction (increase current N component)
                relative_velocity[2] += self.delta_v_magnitude

            # Update the internal state with new relative velocities
            self._current_state[5:8] = relative_velocity

            # --- Simplified Heuristic for propagation effect of maneuver ---
            # This is NOT physically accurate but demonstrates how action could change collision risk.
            # In a real system, an orbital propagator would re-calculate miss_distance and collision_probability.

            current_miss_dist = self._current_state[0]
            current_prob = self._current_state[8]

            # A very simplistic model: small delta-v directly improves miss distance and reduces probability
            # Factor could be tuned or derived from a more complex model
            miss_dist_increase_per_delta_v = 1000.0 # Example: 1000m increase per km/s delta_v
            prob_decrease_factor_per_delta_v = 0.5 # Example: reduce probability by 50% per delta_v

            # Impact of maneuver on miss_distance and collision_probability
            # The effect is proportional to delta_v_magnitude
            self._current_state[0] += miss_dist_increase_per_delta_v * self.delta_v_magnitude # Increase miss distance
            self._current_state[8] *= (1.0 - prob_decrease_factor_per_delta_v * self.delta_v_magnitude) # Reduce collision probability
            self._current_state[9] *= (1.0 - prob_decrease_factor_per_delta_v * self.delta_v_magnitude) # Reduce max collision probability

            # Clip probabilities to valid range [0, 1]
            self._current_state[8] = np.clip(self._current_state[8], 0.0, 1.0)
            self._current_state[9] = np.clip(self._current_state[9], 0.0, 1.0)

            # Penalize for maneuvering (action > 0)
            reward -= 0.1 # Small penalty for fuel consumption

        # Define reward based on the *updated* internal state
        current_prob = self._current_state[8]
        current_miss_dist = self._current_state[0]

        safe_miss_distance_threshold = 100.0 # km
        critical_miss_distance_threshold = 5.0 # km
        safe_prob_threshold = 1e-7 # Very low probability
        high_prob_threshold = 1e-4 # High risk probability

        if current_prob < safe_prob_threshold and current_miss_dist > safe_miss_distance_threshold:
            reward += 10.0 # Significant positive reward for very safe state
            terminated = True
        elif current_prob > high_prob_threshold or current_miss_dist < critical_miss_distance_threshold:
            reward -= 50.0 # Large negative penalty for high risk/collision
            terminated = True
        else:
            # Continuous reward based on inverse probability and miss distance
            # Higher miss_distance is good, lower probability is good
            # Ensure no division by zero for current_prob
            reward += (1.0 / (current_prob + 1e-12)) * 1e-3 + (current_miss_dist / 1000.0) # Scale appropriately

        # Episode termination conditions
        if self._steps_taken_in_episode >= self.max_steps_per_episode:
            truncated = True # Episode ends due to time limit

        # Ensure observation values are within defined bounds after all updates
        self._current_state[0] = np.clip(self._current_state[0], self.observation_space.low[0], self.observation_space.high[0])
        self._current_state[1] = np.clip(self._current_state[1], self.observation_space.low[1], self.observation_space.high[1])
        # Clip relative positions and velocities too, though they are usually unbounded in Box space
        for i in range(2, 8):
            self._current_state[i] = np.clip(self._current_state[i], self.observation_space.low[i], self.observation_space.high[i])
        self._current_state[8] = np.clip(self._current_state[8], self.observation_space.low[8], self.observation_space.high[8])
        self._current_state[9] = np.clip(self._current_state[9], self.observation_space.low[9], self.observation_space.high[9])


        observation = self._get_obs()
        info = self._get_info()

        return observation, reward, terminated, truncated, info

# Instantiate the environment to verify it works
env = SatelliteManeuverEnv(df)
print("Environment created successfully!")
print(f"Observation Space: {env.observation_space}")
print(f"Action Space: {env.action_space}")

# Test a reset and a step
obs, info = env.reset()
print("\nInitial Observation (first conjunction event):", obs)
print("Initial Info:", info)
print("Maneverability of current conjunction: ", info['object1_maneuverable'])

# Take a dummy action (e.g., no maneuver)
action_to_take = 0 # No maneuver
print(f"\nTaking action: {action_to_take} (No maneuver)")
obs, reward, terminated, truncated, info = env.step(action_to_take)
print("Observation after one step:", obs)
print("Reward after one step:", reward)
print("Terminated after one step:", terminated)
print("Truncated after one step:", truncated)
print("Info after one step:", info)

# Take another action (e.g., delta-v in +R direction) if object1 is maneuverable in the current state
if info['object1_maneuverable'] == 'Y':
    action_to_take = 1 # Delta-v in +R
    print(f"\nTaking action: {action_to_take} (Delta-v in +R direction)")
    obs, reward, terminated, truncated, info = env.step(action_to_take)
    print("Observation after second step:", obs)
    print("Reward after second step:", reward)
    print("Terminated after second step:", terminated)
    print("Truncated after second step:", truncated)
    print("Info after second step:", info)
else:
    print("\nSkipping maneuver action test as object1 is not maneuverable in this episode.")

"""## Visualize Initial State

### Subtask:
Generate a visualization of a sample initial state of the RL environment, showing the relative positions or trajectories of 'object1' and 'object2' to help understand the problem setup. Ensure proper legends are provided.

**Reasoning**:
To visualize the initial state of the RL environment, I need to import the necessary plotting libraries, instantiate the environment, reset it to obtain an initial observation, extract the relative position data, and then create a 3D scatter plot showing Object1 at the origin and Object2 at its relative position.
"""

import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# Create an instance of the SatelliteManeuverEnv class
# The 'df' DataFrame is assumed to be available from previous steps
env = SatelliteManeuverEnv(df)

# Reset the environment to get an initial observation
initial_obs, initial_info = env.reset()

# Extract relative position components from the initial observation
# 'miss_distance', 'relative_speed',
# 'relative_position_r', 'relative_position_t', 'relative_position_n',
# 'relative_velocity_r', 'relative_velocity_t', 'relative_velocity_n',
# 'collision_probability', 'collision_max_probability'
relative_position_r = initial_obs[2]
relative_position_t = initial_obs[3]
relative_position_n = initial_obs[4]

# Create a 3D scatter plot
fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')

# Plot Object1 at the origin (0,0,0) as the reference point
ax.scatter(0, 0, 0, color='blue', marker='o', s=100, label='Object1 (Reference)')

# Plot Object2 at its relative position
ax.scatter(relative_position_r, relative_position_t, relative_position_n,
           color='red', marker='^', s=100, label='Object2 (Relative Position)')

# Label the axes
ax.set_xlabel('Relative Position R (km)')
ax.set_ylabel('Relative Position T (km)')
ax.set_zlabel('Relative Position N (km)')

# Add a title to the plot
ax.set_title('Initial Relative Position of Object2 from Object1')

# Add a legend
ax.legend()

# Display the plot
plt.show()

# Ensure the environment class is defined (from previous steps)
# from your_module import SatelliteManeuverEnv # Or re-run the cell where it's defined

# Instantiate the environment (assuming 'df' is available from data loading step)
env = SatelliteManeuverEnv(df)

# Reset the environment to start a new episode
obs, info = env.reset()
print("Initial Observation:", obs)
print("Initial Info:", info)
print("Object1 maneuverable status for this episode:", info['object1_maneuverable'])

# --- Example 1: Taking 'No maneuver' action (action = 0) ---
action_no_maneuver = 0
print(f"\nExecuting action: {action_no_maneuver} (No maneuver)")
new_obs_1, reward_1, terminated_1, truncated_1, info_1 = env.step(action_no_maneuver)

print("Observation after 'No maneuver':", new_obs_1)
print("Reward after 'No maneuver':", reward_1)
print("Terminated:", terminated_1)
print("Truncated:", truncated_1)
print("Info:", info_1)

# --- Example 2: Taking a 'Delta-v in +R direction' action (action = 1) if maneuverable ---
# This will only have a simulated effect if 'object1_maneuverable' is 'Y'
if info['object1_maneuverable'] == 'Y':
    action_delta_v_r = 1
    print(f"\nExecuting action: {action_delta_v_r} (Delta-v in +R direction)")
    new_obs_2, reward_2, terminated_2, truncated_2, info_2 = env.step(action_delta_v_r)

    print("Observation after 'Delta-v +R':", new_obs_2)
    print("Reward after 'Delta-v +R':", reward_2)
    print("Terminated:", terminated_2)
    print("Truncated:", truncated_2)
    print("Info:", info_2)
else:
    print("\nCannot execute a maneuver action in this episode as Object1 is not maneuverable.")

import matplotlib.pyplot as plt
import numpy as np

# Assuming 'env' is already instantiated and 'initial_obs' is available from previous steps
# If not, uncomment and run the following lines:
# env = SatelliteManeuverEnv(df)
# initial_obs, _ = env.reset()

# Extract relative velocity components from the initial observation
# 'relative_velocity_r' is at index 5
# 'relative_velocity_t' is at index 6
# 'relative_velocity_n' is at index 7
relative_velocity_r = initial_obs[5]
relative_velocity_t = initial_obs[6]
relative_velocity_n = initial_obs[7]

velocity_components = [relative_velocity_r, relative_velocity_t, relative_velocity_n]
labels = ['Relative Velocity R (km/s)', 'Relative Velocity T (km/s)', 'Relative Velocity N (km/s)']

# Create a bar plot
fig, ax = plt.subplots(figsize=(8, 6))
ax.bar(labels, velocity_components, color=['skyblue', 'lightcoral', 'lightgreen'])

# Add labels and title
ax.set_ylabel('Velocity (km/s)')
ax.set_title("Object2's Relative Velocity Components from Object1")
ax.grid(axis='y', linestyle='--', alpha=0.7)

# Display the plot
plt.show()

"""## Prepare for Agent Training

### Subtask:
Install the `stable-baselines3` library (if not already installed) which provides robust implementations of various deep reinforcement learning algorithms. This will allow us to easily define and train an agent using deep neural networks.

**Reasoning**:
To prepare for agent training as requested by the subtask, I need to install the `stable-baselines3` library using `pip`.
"""

pip install stable-baselines3[extra]

"""## Initialize RL Environment

### Subtask:
Instantiate the `SatelliteManeuverEnv` with the loaded DataFrame (`df`) to prepare it for agent training.

**Reasoning**:
To complete the subtask, I will instantiate the `SatelliteManeuverEnv` class using the `df` DataFrame, which is already loaded in the kernel.
"""

env = SatelliteManeuverEnv(df)
print("SatelliteManeuverEnv instantiated successfully.")

"""## Choose and Configure RL Agent

### Subtask:
Select a suitable deep reinforcement learning algorithm (e.g., Proximal Policy Optimization - PPO) and configure it with a deep neural network policy (e.g., `MlpPolicy` from `stable-baselines3`) to handle the continuous observation space and discrete action space of the `SatelliteManeuverEnv`.

**Reasoning**:
To select and configure the RL agent, I need to import the PPO algorithm and MlpPolicy from stable_baselines3 and then instantiate the PPO model with the environment.

**Reasoning**:
The `ImportError` indicates that `MlpPolicy` could not be directly imported from `stable_baselines3.common.policies`. A common and robust way to specify the policy network in `stable-baselines3` algorithms like PPO is to pass its name as a string (e.g., `'MlpPolicy'`) instead of importing the class directly. This allows the library to internally map to the correct policy implementation.
"""

from stable_baselines3 import PPO
# No longer importing MlpPolicy directly, as it will be passed as a string

# Instantiate the PPO model
# 'MlpPolicy' (as a string) specifies a Multi-layer Perceptron policy suitable for environments
# with continuous observation spaces and discrete action spaces.
model = PPO("MlpPolicy", env, verbose=1, seed=42)

print("PPO model with MlpPolicy instantiated successfully.")

"""**Reasoning**:
With the RL environment and agent configured, the next logical step is to train the agent using the `learn` method of the `PPO` model. This will allow the agent to interact with the environment, gather experiences, and update its policy to learn optimal maneuvering strategies.


"""

total_timesteps = 10000 # Example: 10,000 timesteps for initial training
model.learn(total_timesteps=total_timesteps)

print(f"Agent trained for {total_timesteps} timesteps.")

"""## Load and Evaluate the Trained Model

### Subtask:
Load the saved RL agent and evaluate its performance in the `SatelliteManeuverEnv`. This will demonstrate the learned behavior and quantify its effectiveness in avoiding collisions and optimizing maneuvers.

**Reasoning**:
To begin the evaluation, I will load the previously saved PPO model using `PPO.load()` from the specified file path.

**Reasoning**:
To evaluate the loaded model, I will create a new instance of the `SatelliteManeuverEnv`, reset it, and then iterate through steps, using the `loaded_model.predict()` to get actions and `env.step()` to apply them, collecting rewards and printing the final outcome.
"""

eval_env = SatelliteManeuverEnv(df)
obs, info = eval_env.reset() # Reset the environment for a new evaluation episode

total_reward = 0
done = False
truncated = False
episode_steps = 0

print("\n--- Starting Evaluation Episode ---")
print(f"Initial Observation: {obs}")
print(f"Initial Info: {info}")
print(f"Object1 maneuverable status: {info['object1_maneuverable']}")

while not done and not truncated:
    action, _states = loaded_model.predict(obs, deterministic=True)
    obs, reward, done, truncated, info = eval_env.step(action)
    total_reward += reward
    episode_steps += 1

    # Optional: Print step-by-step progress for detailed observation
    # print(f"Step {episode_steps}: Action={action}, Reward={reward:.2f}, New Obs={obs}, Done={done}, Truncated={truncated}")

print("\n--- Evaluation Episode Finished ---")
print(f"Total steps taken: {episode_steps}")
print(f"Total reward received: {total_reward:.2f}")
print(f"Final Observation: {obs}")
print(f"Episode terminated: {done}")
print(f"Episode truncated: {truncated}")

"""**Reasoning**:
The previous output included a `DeprecationWarning` from `jupyter_client`. To prevent this warning from appearing in future executions and keep the output clean, I will suppress `DeprecationWarning` specifically from the `jupyter_client` module.


"""

import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning, module='jupyter_client')

eval_env = SatelliteManeuverEnv(df)
obs, info = eval_env.reset() # Reset the environment for a new evaluation episode

total_reward = 0
done = False
truncated = False
episode_steps = 0

print("\n--- Starting Evaluation Episode ---")
print(f"Initial Observation: {obs}")
print(f"Initial Info: {info}")
print(f"Object1 maneuverable status: {info['object1_maneuverable']}")

while not done and not truncated:
    action, _states = loaded_model.predict(obs, deterministic=True)
    obs, reward, done, truncated, info = eval_env.step(action)
    total_reward += reward
    episode_steps += 1

    # Optional: Print step-by-step progress for detailed observation
    # print(f"Step {episode_steps}: Action={action}, Reward={reward:.2f}, New Obs={obs}, Done={done}, Truncated={truncated}")

print("\n--- Evaluation Episode Finished ---")
print(f"Total steps taken: {episode_steps}")
print(f"Total reward received: {total_reward:.2f}")
print(f"Final Observation: {obs}")
print(f"Episode terminated: {done}")
print(f"Episode truncated: {truncated}")

"""## Discuss AI Model Development Integration

### Subtask:
Explain how deep learning models are integral to the RL agent's policy and value functions, and discuss potential advanced integration points with AI model development, such as using unsupervised clustering for state representation or reward shaping in more complex scenarios.

## Final Task

### Subtask:
Summarize the entire process of creating, training, and integrating the reinforcement learning model within the context of AI model development for satellite maneuver optimization.

## Summary:

### Data Analysis Key Findings

*   The `stable-baselines3` library, including its dependencies, was successfully installed, providing the necessary tools for deep reinforcement learning.
*   A `SatelliteManeuverEnv` was successfully instantiated using the prepared DataFrame, creating the environment for the reinforcement learning agent.
*   A Proximal Policy Optimization (PPO) agent was configured with an `MlpPolicy` and successfully trained for 10,000 timesteps on the `SatelliteManeuverEnv`. Initial `ImportError` for `MlpPolicy` and `SyntaxError` during training were resolved.
*   The trained PPO model was successfully saved to a file named `ppo_satellite_maneuver.zip` for persistence.
*   The trained PPO model was successfully loaded and evaluated in the `SatelliteManeuverEnv`. During evaluation, one episode ran for 10 steps, accumulating a total reward of 50254.14 and was truncated. Another evaluation run for a non-maneuverable object completed in 1 step, yielding a total reward of 10.00 and was terminated, demonstrating the agent's interaction and outcome variations.
*   Deep learning models were identified as integral to the RL agent's policy (actor) and value (critic) functions, enabling generalization and handling complex state spaces.
*   Several advanced AI integration points were discussed, including using unsupervised clustering for state representation, adaptive reward shaping, transfer learning from pre-trained models, and integrating predictive models for dynamic reward adjustment, all aimed at enhancing agent performance and robustness.

### Insights or Next Steps

*   **Further Evaluate and Refine Agent Performance**: The varied evaluation results (high reward in one episode, low in another for a non-maneuverable object) suggest the need for more comprehensive evaluation across diverse conjunction scenarios to assess the agent's generalization capabilities and identify scenarios where it struggles. This can involve running multiple evaluation episodes with different initial states.
*   **Implement Advanced Integration Techniques**: Explore incorporating one or more of the discussed advanced AI model development techniques, such as adaptive reward shaping based on domain knowledge or expert demonstrations, or integrating a more accurate orbital propagation model, to significantly enhance the agent's learning efficiency, physical realism, and overall performance in avoiding collisions.

# Task
Create `agent_modules.json` to configure the RL agent, including observation/action space definitions, reward function parameters, and algorithm settings. Then, create `model.py` to encapsulate the `SatelliteManeuverEnv` class and provide functionality for loading and using the trained `stable-baselines3` PPO model for making predictions. Finally, summarize the contents and purpose of both files, explaining their role in the overall agent setup for satellite maneuver optimization.

## Create Agent Modules JSON

### Subtask:
Create a new file named `agent_modules.json`. This JSON file will define the structure and content for configuring various components of the RL agent, such as policy network parameters, reward function weights, or other settings that could be loaded dynamically.

**Reasoning**:
To create the `agent_modules.json` file with the specified content, I will use Python's `json` module to dump a dictionary representing the configuration into the file.
"""

import json

agent_config = {
    "observation_space": {
        "features": [
            "miss_distance",
            "relative_speed",
            "relative_position_r",
            "relative_position_t",
            "relative_position_n",
            "relative_velocity_r",
            "relative_velocity_t",
            "relative_velocity_n",
            "collision_probability",
            "collision_max_probability"
        ],
        "bounds": {
            "low": [0.0, 0.0, -500.0, -500.0, -500.0, -8000.0, -17000.0, -10000.0, 0.0, 0.0],
            "high": [500.0, 17000.0, 500.0, 500.0, 500.0, 3500.0, 1500.0, 10000.0, 1.0, 1.0]
        }
    },
    "action_space": {
        "type": "Discrete",
        "n_actions": 7,
        "delta_v_magnitude": 0.01,
        "actions_map": {
            "0": "no_maneuver",
            "1": "+R",
            "2": "-R",
            "3": "+T",
            "4": "-T",
            "5": "+N",
            "6": "-N"
        }
    },
    "reward_function_parameters": {
        "safe_miss_distance_threshold": 100.0,
        "critical_miss_distance_threshold": 5.0,
        "safe_prob_threshold": 1e-7,
        "high_prob_threshold": 1e-4,
        "positive_reward_safe_state": 10.0,
        "negative_penalty_high_risk": -50.0,
        "maneuver_cost": -0.1,
        "miss_dist_increase_per_delta_v": 1000.0,
        "prob_decrease_factor_per_delta_v": 0.5
    },
    "algorithm_settings": {
        "agent_type": "PPO",
        "policy_type": "MlpPolicy",
        "total_timesteps": 10000,
        "learning_rate": 0.0003,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "n_steps": 2048,
        "clip_range": 0.2,
        "batch_size": 64
    }
}

file_path = "agent_modules.json"
with open(file_path, "w") as f:
    json.dump(agent_config, f, indent=4)

print(f"Successfully created {file_path} with the specified configuration.")

"""**Reasoning**:
The previous command successfully created the `agent_modules.json` file. Now, the next instruction is to create `model.py` to encapsulate the `SatelliteManeuverEnv` class and functionality for loading and using the trained `stable-baselines3` PPO model. I will start by defining the `SatelliteManeuverEnv` class within `model.py`.


"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile model.py
# import gymnasium as gym
# import numpy as np
# import pandas as pd
# from stable_baselines3 import PPO
# import json
# import os
# 
# class SatelliteManeuverEnv(gym.Env):
#     metadata = {'render_modes': ['human'], 'render_fps': 30}
# 
#     def __init__(self, df):
#         super().__init__()
#         self.df = df.copy() # Work on a copy to avoid modifying original df
#         self.num_conjunctions = len(df)
#         self.max_steps_per_episode = 10 # Define maximum steps to simulate a single conjunction event
# 
#         # Handle NaN in 'object1_maneuverable' by filling with 'N'
#         if 'object1_maneuverable' in self.df.columns:
#             self.df['object1_maneuverable'] = self.df['object1_maneuverable'].fillna('N').astype(str)
#         else:
#             # If column doesn't exist, assume not maneuverable for all
#             self.df['object1_maneuverable'] = 'N'
# 
#         # Define observation space based on relevant columns
#         feature_columns = ['miss_distance', 'relative_speed',
#                          'relative_position_r', 'relative_position_t', 'relative_position_n',
#                          'relative_velocity_r', 'relative_velocity_t', 'relative_velocity_n',
#                          'collision_probability', 'collision_max_probability']
# 
#         # Calculate bounds and explicitly cast to float32 to avoid UserWarning
#         low_bounds = self.df[feature_columns].min().values.astype(np.float32)
#         high_bounds = self.df[feature_columns].max().values.astype(np.float32)
# 
#         # Ensure logical bounds for specific features
#         low_bounds[0] = 0.0 # miss_distance cannot be negative
#         low_bounds[1] = 0.0 # relative_speed cannot be negative
#         low_bounds[8] = 0.0 # collision_probability min is 0
#         low_bounds[9] = 0.0 # collision_max_probability min is 0
#         high_bounds[8] = 1.0 # collision_probability max is 1
#         high_bounds[9] = 1.0 # collision_max_probability max is 1
# 
#         # Ensure all values are finite before creating the Box space, converting inf to float32 max/min
#         low_bounds = np.nan_to_num(low_bounds, nan=-np.inf, posinf=np.finfo(np.float32).max, neginf=np.finfo(np.float32).min)
#         high_bounds = np.nan_to_num(high_bounds, nan=np.inf, posinf=np.finfo(np.float32).max, neginf=np.finfo(np.float32).min)
# 
#         self.observation_space = gym.spaces.Box(low=low_bounds, high=high_bounds, dtype=np.float32)
# 
#         # Define action space:
#         # 0: no maneuver
#         # 1-2: delta-v in R (positive/negative)
#         # 3-4: delta-v in T (positive/negative)
#         # 5-6: delta-v in N (positive/negative)
#         self.delta_v_magnitude = 0.01 # Adjusted example magnitude in km/s (smaller, more realistic)
#         self.action_space = gym.spaces.Discrete(7) # 7 discrete actions
# 
#         # Internal state that gets updated within an episode
#         self._current_state = None
#         self._current_conjunction_idx = None # Index of the conjunction from df for the current episode
#         self._steps_taken_in_episode = 0
# 
#     def _get_obs(self):
#         # Return a copy to prevent external modification of internal state
#         return self._current_state.copy()
# 
#     def _get_info(self):
#         # Ensure 'object1_maneuverable' is a string 'Y' or 'N'
#         maneuverable_status = self.df.iloc[self._current_conjunction_idx]['object1_maneuverable']
#         return {'object1_maneuverable': str(maneuverable_status)}
# 
#     def reset(self, seed=None, options=None):
#         super().reset(seed=seed)
#         # Randomly select a conjunction event from the DataFrame to start a new episode
#         self._current_conjunction_idx = self.np_random.integers(self.num_conjunctions)
#         self._steps_taken_in_episode = 0
# 
#         # Initialize the internal mutable state for the episode
#         current_event_data = self.df.iloc[self._current_conjunction_idx]
# 
#         feature_columns = ['miss_distance', 'relative_speed',
#                          'relative_position_r', 'relative_position_t', 'relative_position_n',
#                          'relative_velocity_r', 'relative_velocity_t', 'relative_velocity_n',
#                          'collision_probability', 'collision_max_probability']
#         self._current_state = current_event_data[feature_columns].values.astype(np.float32)
# 
#         # Ensure probabilities are within [0, 1] bounds, and other physical bounds
#         self._current_state[8] = np.clip(self._current_state[8], 0.0, 1.0) # collision_probability
#         self._current_state[9] = np.clip(self._current_state[9], 0.0, 1.0) # collision_max_probability
#         self._current_state[0] = np.maximum(self._current_state[0], 0.0) # miss_distance
#         self._current_state[1] = np.maximum(self._current_state[1], 0.0) # relative_speed
# 
#         observation = self._get_obs()
#         info = self._get_info()
#         return observation, info
# 
#     def step(self, action):
#         self._steps_taken_in_episode += 1
#         terminated = False
#         truncated = False
#         reward = 0.0
# 
#         maneuverable = self._get_info()['object1_maneuverable'] == 'Y'
# 
#         # Apply delta-v based on action if object1 is maneuverable
#         if maneuverable and action > 0:
#             # A temporary copy of velocity components from the current internal state
#             relative_velocity = self._current_state[5:8].copy() # Indices for r_v, t_v, n_v
# 
#             if action == 1: # +R direction (reduce current R component)
#                 relative_velocity[0] -= self.delta_v_magnitude
#             elif action == 2: # -R direction (increase current R component)
#                 relative_velocity[0] += self.delta_v_magnitude
#             elif action == 3: # +T direction (reduce current T component)
#                 relative_velocity[1] -= self.delta_v_magnitude
#             elif action == 4: # -T direction (increase current T component)
#                 relative_velocity[1] += self.delta_v_magnitude
#             elif action == 5: # +N direction (reduce current N component)
#                 relative_velocity[2] -= self.delta_v_magnitude
#             elif action == 6: # -N direction (increase current N component)
#                 relative_velocity[2] += self.delta_v_magnitude
# 
#             # Update the internal state with new relative velocities
#             self._current_state[5:8] = relative_velocity
# 
#             # --- Simplified Heuristic for propagation effect of maneuver ---
#             # This is NOT physically accurate but demonstrates how action could change collision risk.
#             # In a real system, an orbital propagator would re-calculate miss_distance and collision_probability.
# 
#             current_miss_dist = self._current_state[0]
#             current_prob = self._current_state[8]
# 
#             # A very simplistic model: small delta-v directly improves miss distance and reduces probability
#             # Factor could be tuned or derived from a more complex model
#             miss_dist_increase_per_delta_v = 1000.0 # Example: 1000m increase per km/s delta_v
#             prob_decrease_factor_per_delta_v = 0.5 # Example: reduce probability by 50% per delta_v
# 
#             # Impact of maneuver on miss_distance and collision_probability
#             # The effect is proportional to delta_v_magnitude
#             self._current_state[0] += miss_dist_increase_per_delta_v * self.delta_v_magnitude # Increase miss distance
#             self._current_state[8] *= (1.0 - prob_decrease_factor_per_delta_v * self.delta_v_magnitude) # Reduce collision probability
#             self._current_state[9] *= (1.0 - prob_decrease_factor_per_delta_v * self.delta_v_magnitude) # Reduce max collision probability
# 
#             # Clip probabilities to valid range [0, 1]
#             self._current_state[8] = np.clip(self._current_state[8], 0.0, 1.0)
#             self._current_state[9] = np.clip(self._current_state[9], 0.0, 1.0)
# 
#             # Penalize for maneuvering (action > 0)
#             reward -= 0.1 # Small penalty for fuel consumption
# 
#         # Define reward based on the *updated* internal state
#         current_prob = self._current_state[8]
#         current_miss_dist = self._current_state[0]
# 
#         safe_miss_distance_threshold = 100.0 # km
#         critical_miss_distance_threshold = 5.0 # km
#         safe_prob_threshold = 1e-7 # Very low probability
#         high_prob_threshold = 1e-4 # High risk probability
# 
#         if current_prob < safe_prob_threshold and current_miss_dist > safe_miss_distance_threshold:
#             reward += 10.0 # Significant positive reward for very safe state
#             terminated = True
#         elif current_prob > high_prob_threshold or current_miss_dist < critical_miss_distance_threshold:
#             reward -= 50.0 # Large negative penalty for high risk/collision
#             terminated = True
#         else:
#             # Continuous reward based on inverse probability and miss distance
#             # Higher miss_distance is good, lower probability is good
#             # Ensure no division by zero for current_prob
#             reward += (1.0 / (current_prob + 1e-12)) * 1e-3 + (current_miss_dist / 1000.0) # Scale appropriately
# 
#         # Episode termination conditions
#         if self._steps_taken_in_episode >= self.max_steps_per_episode:
#             truncated = True # Episode ends due to time limit
# 
#         # Ensure observation values are within defined bounds after all updates
#         self._current_state[0] = np.clip(self._current_state[0], self.observation_space.low[0], self.observation_space.high[0])
#         self._current_state[1] = np.clip(self._current_state[1], self.observation_space.low[1], self.observation_space.high[1])
#         # Clip relative positions and velocities too, though they are usually unbounded in Box space
#         for i in range(2, 8):
#             self._current_state[i] = np.clip(self._current_state[i], self.observation_space.low[i], self.observation_space.high[i])
#         self._current_state[8] = np.clip(self._current_state[8], self.observation_space.low[8], self.observation_space.high[8])
#         self._current_state[9] = np.clip(self._current_state[9], self.observation_space.low[9], self.observation_space.high[9])
# 
#         observation = self._get_obs()
#         info = self._get_info()
# 
#         return observation, reward, terminated, truncated, info
# 
# # Function to load model and make predictions
# def load_and_predict(model_path, df, config_path="agent_modules.json", num_eval_episodes=5):
#     # Load configuration
#     with open(config_path, 'r') as f:
#         config = json.load(f)
# 
#     # Instantiate the environment
#     env = SatelliteManeuverEnv(df)
# 
#     # Load the trained model
#     loaded_model = PPO.load(model_path, env=env)
# 
#     episode_rewards = []
#     for i in range(num_eval_episodes):
#         obs, info = env.reset()
#         total_reward = 0
#         done = False
#         truncated = False
#         episode_steps = 0
# 
#         print(f"\n--- Starting Evaluation Episode {i+1} ---")
#         print(f"Initial Observation: {obs}")
#         print(f"Initial Info: {info}")
# 
#         while not done and not truncated:
#             action, _states = loaded_model.predict(obs, deterministic=True)
#             obs, reward, done, truncated, info = env.step(action)
#             total_reward += reward
#             episode_steps += 1
# 
#         episode_rewards.append(total_reward)
#         print(f"--- Evaluation Episode {i+1} Finished ---")
#         print(f"Total steps taken: {episode_steps}")
#         print(f"Total reward received: {total_reward:.2f}")
#         print(f"Final Observation: {obs}")
#         print(f"Episode terminated: {done}")
#         print(f"Episode truncated: {truncated}")
# 
#     print(f"\nAverage reward over {num_eval_episodes} episodes: {np.mean(episode_rewards):.2f}")
#     return loaded_model, np.mean(episode_rewards)
# 
# print("model.py created successfully with SatelliteManeuverEnv class and load_and_predict function.")
#

"""## Summary of `agent_modules.json` and `model.py`

### `agent_modules.json`

**Contents:** This file is a JSON document that stores configuration settings for the Reinforcement Learning (RL) agent and its environment. It includes:
*   `observation_space`: Defines the features that constitute the agent's state (e.g., `miss_distance`, `relative_velocity_r`) and their corresponding numerical bounds.
*   `action_space`: Specifies the type of action space (Discrete), the number of available actions, the magnitude of `delta-v` for maneuvers, and a mapping of action indices to descriptive maneuver names.
*   `reward_function_parameters`: Contains all the thresholds and weights used in calculating the reward for the agent, such as safe distance, collision probability thresholds, positive rewards, negative penalties, and maneuver costs.
*   `algorithm_settings`: Details the parameters for the chosen RL algorithm (PPO), including `agent_type`, `policy_type`, `total_timesteps` for training, `learning_rate`, `gamma`, and other hyperparameters.

**Purpose:** The `agent_modules.json` file serves as a centralized and easily modifiable configuration store for the RL setup. By externalizing these parameters, we can:
*   **Enhance Modularity:** Keep environment and agent parameters separate from the code, making the system more organized.
*   **Facilitate Experimentation:** Easily tune hyperparameters and reward function components without altering the core Python code.
*   **Improve Reproducibility:** Ensure consistent environment and agent setups across different runs or team members by loading a standardized configuration.

### `model.py`

**Contents:** This Python file encapsulates two main components:
*   **`SatelliteManeuverEnv` Class:** This class defines the custom Gymnasium environment for the satellite maneuver optimization problem. It inherits from `gymnasium.Env` and implements the core RL environment functionalities:
    *   `__init__`: Initializes the environment, processes the input DataFrame, handles maneuverability status, and defines the observation and action spaces using parameters derived from the DataFrame and `agent_modules.json` (though currently hardcoded, it's designed for dynamic loading).
    *   `_get_obs`: Returns the current state observation.
    *   `_get_info`: Provides additional information about the current state, notably `object1_maneuverable` status.
    *   `reset`: Resets the environment to a new, randomly selected conjunction event for the start of an episode.
    *   `step`: Executes an agent's action, updates the internal state based on simplified maneuver heuristics, calculates the reward, and determines if the episode is terminated or truncated.
*   **`load_and_predict` Function:** This utility function is designed to:
    *   Load the configuration from `agent_modules.json`.
    *   Instantiate the `SatelliteManeuverEnv`.
    *   Load a trained `stable-baselines3` PPO model from a specified path.
    *   Evaluate the loaded model over a given number of episodes, printing the initial state, actions taken, rewards, and final episode status.

**Purpose:** The `model.py` file centralizes the environment logic and provides the necessary tools for interacting with a trained RL agent. Its purpose is to:
*   **Encapsulate Environment Logic:** Keep the complex simulation dynamics and reward calculations within a single, reusable class.
*   **Provide a Standard Interface:** Adhere to the Gymnasium API, allowing compatibility with `stable-baselines3` and other RL frameworks.
*   **Facilitate Model Deployment and Evaluation:** Offer a straightforward way to load a pre-trained agent and evaluate its performance in the defined environment, which is crucial for testing and deploying the learned policy in real-world or simulated operational scenarios.

## Summary of `agent_modules.json` and `model.py`

### `agent_modules.json`

**Contents:** This file is a JSON document that stores configuration settings for the Reinforcement Learning (RL) agent and its environment. It includes:
*   `observation_space`: Defines the features that constitute the agent's state (e.g., `miss_distance`, `relative_velocity_r`) and their corresponding numerical bounds.
*   `action_space`: Specifies the type of action space (Discrete), the number of available actions, the magnitude of `delta-v` for maneuvers, and a mapping of action indices to descriptive maneuver names.
*   `reward_function_parameters`: Contains all the thresholds and weights used in calculating the reward for the agent, such as safe distance, collision probability thresholds, positive rewards, negative penalties, and maneuver costs.
*   `algorithm_settings`: Details the parameters for the chosen RL algorithm (PPO), including `agent_type`, `policy_type`, `total_timesteps` for training, `learning_rate`, `gamma`, and other hyperparameters.

**Purpose:** The `agent_modules.json` file serves as a centralized and easily modifiable configuration store for the RL setup. By externalizing these parameters, we can:
*   **Enhance Modularity:** Keep environment and agent parameters separate from the code, making the system more organized.
*   **Facilitate Experimentation:** Easily tune hyperparameters and reward function components without altering the core Python code.
*   **Improve Reproducibility:** Ensure consistent environment and agent setups across different runs or team members by loading a standardized configuration.

### `model.py`

**Contents:** This Python file encapsulates two main components:
*   **`SatelliteManeuverEnv` Class:** This class defines the custom Gymnasium environment for the satellite maneuver optimization problem. It inherits from `gymnasium.Env` and implements the core RL environment functionalities:
    *   `__init__`: Initializes the environment, processes the input DataFrame, handles maneuverability status, and defines the observation and action spaces using parameters derived from the DataFrame and `agent_modules.json` (though currently hardcoded, it's designed for dynamic loading).
    *   `_get_obs`: Returns the current state observation.
    *   `_get_info`: Provides additional information about the current state, notably `object1_maneuverable` status.
    *   `reset`: Resets the environment to a new, randomly selected conjunction event for the start of an episode.
    *   `step`: Executes an agent's action, updates the internal state based on simplified maneuver heuristics, calculates the reward, and determines if the episode is terminated or truncated.
*   **`load_and_predict` Function:** This utility function is designed to:
    *   Load the configuration from `agent_modules.json`.
    *   Instantiate the `SatelliteManeuverEnv`.
    *   Load a trained `stable-baselines3` PPO model from a specified path.
    *   Evaluate the loaded model over a given number of episodes, printing the initial state, actions taken, rewards, and final episode status.

**Purpose:** The `model.py` file centralizes the environment logic and provides the necessary tools for interacting with a trained RL agent. Its purpose is to:
*   **Encapsulate Environment Logic:** Keep the complex simulation dynamics and reward calculations within a single, reusable class.
*   **Provide a Standard Interface:** Adhere to the Gymnasium API, allowing compatibility with `stable-baselines3` and other RL frameworks.
*   **Facilitate Model Deployment and Evaluation:** Offer a straightforward way to load a pre-trained agent and evaluate its performance in the defined environment, which is crucial for testing and deploying the learned policy in real-world or simulated operational scenarios.

## Final Task

### Subtask:
Summarize the created `agent_modules.json` and `model.py` files, explaining their purpose and how they contribute to the overall agent setup for reinforcement learning environments for maneuver optimization.

## Summary:

### Data Analysis Key Findings

*   The `agent_modules.json` file was successfully created to act as a centralized configuration store for the Reinforcement Learning (RL) agent. It defines the observation space, action space (7 discrete actions including no maneuver and +/- maneuvers in R, T, N directions with a $\Delta$V magnitude of 0.01), reward function parameters (e.g., safe miss distance threshold of 100.0 km, critical miss distance threshold of 5.0 km, high probability threshold of 1e-4), and PPO algorithm settings (e.g., `total_timesteps`: 10000, `learning_rate`: 0.0003, `gamma`: 0.99).
*   The `model.py` file was successfully created, encapsulating the `SatelliteManeuverEnv` class and the `load_and_predict` function.
    *   The `SatelliteManeuverEnv` class provides a custom Gymnasium environment for satellite maneuver optimization, adhering to the Gymnasium API for compatibility with RL frameworks like `stable-baselines3`. It includes features such as defining observation and action spaces, handling conjunction event resets, and executing steps with simplified maneuver heuristics and reward calculations.
    *   The `load_and_predict` function in `model.py` is designed to load configuration from `agent_modules.json`, instantiate the environment, load a trained `stable-baselines3` PPO model, and evaluate its performance over a specified number of episodes.

### Insights or Next Steps

*   The clear separation of configuration in `agent_modules.json` and environment/model interaction logic in `model.py` significantly enhances the modularity, maintainability, and experimental flexibility of the RL agent setup.
*   The `load_and_predict` function provides a crucial framework for evaluating trained RL models, making it straightforward to assess an agent's performance in a simulated environment and prepare for deployment or further testing.

# Task
Implement Layer-wise Relevance Propagation (LRP) using Captum to interpret the deep learning model's predictions, identifying which input features (e.g., specific relative positions or velocities) contribute most to the agent's decisions.

## Load Model

### Subtask:
Load the previously saved reinforcement learning model from disk, making it ready for exploitation and evaluation.

**Reasoning**:
To load the previously saved reinforcement learning model, I need to import the `PPO` class from `stable_baselines3` and then use its `load()` method with the provided model file path. The loaded model will be stored in a variable and a confirmation message will be printed.

**Reasoning**:
To implement LRP, I need to extract the policy network (actor) from the loaded PPO model, which is a PyTorch module. Then, I'll define a forward function for Captum that wraps the model's prediction logic to obtain the action logits, which are the direct outputs before sampling or argmax, for interpretability.

**Reasoning**:
The previous code block failed because the `captum` library was not installed. I need to install `captum` using `pip` before attempting to import and use it.
"""

pip install captum

"""**Reasoning**:
Since the `captum` library is now installed, I will re-execute the code to extract the policy network from the loaded PPO model and wrap it for Captum LRP analysis.

**Reasoning**:
The previous code failed because `LayerwiseRelevancePropagation` could not be imported directly from `captum.attr`. This typically happens when the class is aliased under a different name or located in a submodule not directly exposed in the top-level `captum.attr` namespace. Checking common Captum usage, the class is often aliased as `LRP` or is available through a more specific path. I will correct the import statement to use `LRP` which is the common alias for `LayerwiseRelevancePropagation` within `captum.attr`.

**Reasoning**:
The `AttributeError: 'ActorCriticPolicy' object has no attribute 'actor'` indicates that `loaded_model.policy` does not have a direct `actor` attribute. In `stable-baselines3`'s PPO, the `loaded_model.policy` itself is the `ActorCriticPolicy` instance, and it contains the logic to compute action distributions. I need to modify the code to directly use `loaded_model.policy` as the network to wrap and access its `get_distribution` method to extract the action `logits` for Captum.
"""

import torch
from captum.attr import LRP # Corrected import name

# The PPO model's policy itself is the network that handles observations
# `loaded_model.policy` is an instance of ActorCriticPolicy.
policy_network = loaded_model.policy

# Ensure the policy network is in evaluation mode
policy_network.eval()

# Define a custom forward function for Captum to interpret
class PolicyOutputWrapper(torch.nn.Module):
    def __init__(self, ppo_policy_module):
        super().__init__()
        self.ppo_policy_module = ppo_policy_module

    def forward(self, observation):
        # Ensure the observation is a batch, even if it's a single sample, as SB3 expects batched inputs
        if observation.dim() == 1:
            observation = observation.unsqueeze(0)

        # Get the action distribution from the PPO policy and extract its logits
        # `get_distribution` is a method of ActorCriticPolicy
        action_distribution = self.ppo_policy_module.get_distribution(observation)
        return action_distribution.logits

# Create an instance of the wrapper, passing the entire loaded_model.policy
wrapped_policy_network = PolicyOutputWrapper(policy_network)

print("Policy network extracted and wrapped for Captum LRP.")

"""**Reasoning**:
Now that the policy network is correctly extracted and wrapped for Captum, I will initialize the LRP explainer, obtain a sample observation from the environment, and then compute the LRP attributions for this observation. After getting attributions, I will display them.

**Reasoning**:
The `NameError: name 'wrapped_policy_network' is not defined` indicates that the variable was not accessible in the current cell's scope, likely due to a kernel reset or unexecuted preceding cells. To fix this, I will include all necessary imports and the full definition of `loaded_model`, `policy_network`, and `wrapped_policy_network` within this code block, making it self-contained and robust.

**Reasoning**:
The `ModuleNotFoundError: No module named 'numpy._core.numeric'` during model loading, coupled with the previous `pip install captum` downgrading `numpy` from `2.0.2` to `1.26.4`, indicates a version incompatibility. The `stable-baselines3` model was likely saved with `numpy 2.0.2`. To resolve this, I need to reinstall `numpy` at version `2.0.2` to ensure compatibility for loading the model. This might cause a conflict with `captum`, which I will address if it arises.
"""

pip install numpy==2.0.2

"""**Reasoning**:
Now that `numpy==2.0.2` is installed, which should resolve the `ModuleNotFoundError` for model loading, I will re-execute the code block to perform LRP analysis. This will test if the `stable-baselines3` model can now be loaded and if `captum` (despite its stated dependency conflict) can function with `numpy 2.0.2`.

**Reasoning**:
The `KeyError` indicates that `PPO.load()` requires the environment to verify its observation and action spaces. I will instantiate the `SatelliteManeuverEnv` before calling `PPO.load()` and pass this environment instance as the `env` argument to resolve the loading issue.

**Reasoning**:
The previous code block failed because the `df` DataFrame, which is essential for initializing the `SatelliteManeuverEnv`, was not defined in the current scope. I need to re-load the `df` DataFrame from the CSV file before instantiating the environment to make it accessible for the subsequent steps.

**Reasoning**:
The previous code block failed with a `KeyError` indicating that the `observation_space` and `action_space` were not found or could not be deserialized from the saved model, despite providing the environment. This often happens due to `numpy` version mismatches during saving and loading, leading to `cloudpickle` deserialization issues. To resolve this, I will explicitly pass the environment's `observation_space` and `action_space`, along with other potentially problematic internal states (`_last_obs`, `_last_episode_starts`), as `custom_objects` to `PPO.load()`. This instructs `stable-baselines3` to use these provided objects during deserialization, overriding any corrupted or incompatible ones from the saved file.

**Reasoning**:
The previous code failed because Captum's LRP could not find a rule for the `torch.nn.modules.flatten.Flatten` layer within the policy network. To resolve this, I will explicitly add a default `IdentityRule` for `torch.nn.Flatten` modules using `captum.attr.add_rules()`. This will ensure LRP can process all layers in the network.

**Reasoning**:
The previous code failed with an `ImportError` for `add_rules` because this function is not directly exposed by `captum.attr`. To fix this, I will remove `add_rules` from the import and instead pass a `module_rules` dictionary directly to the `LRP` constructor, specifying `IdentityRule` for `torch.nn.Flatten` layers. This is the correct way to define custom rules for `LRP` in the installed Captum version.

**Reasoning**:
The previous code failed with an `ImportError` because `IdentityRule` is not directly available under `captum.attr`. Based on Captum's structure, `IdentityRule` is typically found within `captum.attr._core.lrp_rules`. I will correct the import statement to reflect this path.

## Implement Layer-wise Relevance Propagation (LRP)

### Subtask:
Correct the import path for `IdentityRule` within Captum to resolve the `ModuleNotFoundError`.

**Reasoning**:
The previous code failed with an `ImportError` because `IdentityRule` is not directly available under `captum.attr`. Based on Captum's documentation for version 0.8.0, `IdentityRule` is located in `captum.attr.baselines.lrp`. I will correct the import path to resolve this issue.

**Reasoning**:
The previous attempt to import `IdentityRule` from `captum.attr.baselines.lrp` failed with a `ModuleNotFoundError`. As per the instructions, I will now try importing `IdentityRule` directly from `captum.attr` to determine if it is exposed at that level.

**Reasoning**:
The previous attempts to import `IdentityRule` directly from `captum.attr` or `captum.attr.baselines.lrp` resulted in `ImportError` or `ModuleNotFoundError`. Based on common Captum structure for LRP rules, `IdentityRule` is usually located within `captum.attr._core.lrp_rules`. I will correct the import statement to reflect this path, as it is the most semantically appropriate location for such a rule.

**Reasoning**:
The previous attempts to import `IdentityRule` from `captum.attr.baselines.lrp` and `captum.attr._core.lrp_rules` both resulted in `ModuleNotFoundError` or `ImportError`. As instructed, I will try importing `IdentityRule` directly from `captum.attr` to see if it is exposed at that level for the installed Captum version.

**Reasoning**:
The previous attempts to import `IdentityRule` from various submodules of `captum.attr` resulted in `ModuleNotFoundError` or `ImportError`. Based on the Captum 0.8.0 source code, `IdentityRule` is located in `captum.attr._utils.lrp_rules`. I will correct the import path to this specific location to resolve the import error.

**Reasoning**:
The previous code failed because the `LRP` constructor in Captum 0.8.0 does not accept the `module_rules` argument. The correct argument name for providing custom layer rules is `rule_configs`. I will update the argument name in the `LRP` constructor and ensure `IdentityRule` is imported from `captum.attr._utils.lrp_rules` as confirmed by previous kernel state.

## Implement Layer-wise Relevance Propagation (LRP)

### Subtask:
Correct the signature of the `_manipulate_weights` method in `CustomFlattenRule` to match the expected arguments from Captum's `PropagationRule` base class.

**Reasoning**:
The previous code failed with a TypeError because the `_manipulate_weights` method in `CustomFlattenRule` had an incorrect signature. I will update the method signature to include `inputs` and `outputs` as arguments, as specified in the instructions, to match the `PropagationRule` base class's expected signature.
"""

import pandas as pd
from captum.attr import LRP
from captum.attr._core.lrp import PropagationRule # Assuming PropagationRule base class path
import numpy as np
import matplotlib.pyplot as plt
import torch
from stable_baselines3 import PPO
from model import SatelliteManeuverEnv # Import the environment class
from typing import Tuple # Import Tuple

# Re-load the DataFrame as it was not defined in the current scope
df = pd.read_csv('/content/CDM_DATA_YOTA.csv', low_memory=False)

# Instantiate the environment before loading the model
eval_env = SatelliteManeuverEnv(df) # Create a new env instance if needed

# Define custom_objects to handle deserialization issues, especially with numpy version changes
custom_objects = {
    "observation_space": eval_env.observation_space,
    "action_space": eval_env.action_space,
    "_last_obs": None, # Set to None to prevent deserialization issues if not critical
    "_last_episode_starts": None, # Set to None to prevent deserialization issues if not critical
}

# --- Start of code needed to define wrapped_policy_network ---
# Load the trained model (from previous successful steps)
# Pass the instantiated environment and custom_objects to PPO.load()
loaded_model = PPO.load("ppo_satellite_maneuver.zip", env=eval_env, custom_objects=custom_objects)

# The PPO model's policy itself is the network that handles observations
# `loaded_model.policy` is an instance of ActorCriticPolicy.
policy_network = loaded_model.policy

# Ensure the policy network is in evaluation mode
policy_network.eval()

# Define a custom forward function for Captum to interpret
class PolicyOutputWrapper(torch.nn.Module):
    def __init__(self, ppo_policy_module):
        super().__init__()
        # Our observations are already 1D and flat (shape 10,). The features_extractor for MlpPolicy
        # in SB3 typically contains a Flatten layer which can be problematic for LRP/graph viz.
        # Since the observation is already flat, we can directly use mlp_extractor and action_net.
        self.mlp_extractor = ppo_policy_module.mlp_extractor
        self.action_net = ppo_policy_module.action_net

    def forward(self, observation):
        # Ensure the observation is a batch, even if it's a single sample, as SB3 expects batched inputs
        if observation.dim() == 1:
            observation = observation.unsqueeze(0)

        # Pass the observation (already treated as flat features) directly to the mlp_extractor
        latent_pi, _ = self.mlp_extractor(observation)

        # Get logits from the action network
        action_logits = self.action_net(latent_pi)
        return action_logits

# Create an instance of the wrapper, passing the entire loaded_model.policy
wrapped_policy_network = PolicyOutputWrapper(policy_network)
# --- End of code needed to define wrapped_policy_network ---

# Define a custom rule for Flatten layers, inheriting from PropagationRule
# and implementing all abstract methods.
class CustomFlattenRule(PropagationRule):
    def propagate(self, relevance: torch.Tensor, layer: torch.nn.Module, input: Tuple[torch.Tensor, ...], output: torch.Tensor) -> torch.Tensor:
        # For a Flatten layer, the relevance is simply reshaped back to the input shape.
        input_tensor_shape = input[0].shape
        return relevance.reshape(input_tensor_shape)

    def _manipulate_weights(self, module: torch.nn.Module, inputs: Tuple[torch.Tensor, ...], outputs: Tuple[torch.Tensor, ...]) -> None:
        # Flatten layers do not have weights to manipulate, so this is a no-op.
        pass

    def _grad_input_hook(self, grad: torch.Tensor) -> torch.Tensor:
        # For Identity-like operations, the gradient is passed through directly.
        return grad

    def _register_hooks(self, module: torch.nn.Module) -> None:
        # No specific hooks needed for a Flatten layer beyond default LRP behavior.
        pass

    def _remove_hooks(self, module: torch.nn.Module) -> None:
        # No specific hooks to remove.
        pass

# Manually assign CustomFlattenRule to Flatten layers within the wrapped policy network
for name, module in wrapped_policy_network.named_modules():
    if isinstance(module, torch.nn.Flatten):
        module.rule = CustomFlattenRule()

# Initialize the LRP explainer without rule arguments
lrp = LRP(wrapped_policy_network)

# Get a sample observation from the environment
# Reset the environment to get a new initial observation for LRP
sample_obs_np, _ = eval_env.reset()
sample_obs_tensor = torch.tensor(sample_obs_np, dtype=torch.float32).unsqueeze(0) # Add batch dimension

# Compute LRP attributions for each possible action output
# target specifies the index of the output (action) for which attribution is computed
attributions_per_action = []
num_actions = eval_env.action_space.n # Use eval_env for action space size

for action_idx in range(num_actions):
    # Captum's LRP requires the target argument for multi-output models
    attributions = lrp.attribute(sample_obs_tensor, target=action_idx)
    attributions_per_action.append(attributions.squeeze().detach().numpy())

# Feature names for better interpretability
feature_names = [
    'miss_distance', 'relative_speed',
    'relative_position_r', 'relative_position_t', 'relative_position_n',
    'relative_velocity_r', 'relative_velocity_t', 'relative_velocity_n',
    'collision_probability', 'collision_max_probability'
]

# Visualize attributions for a selected action (e.g., the predicted action)
# First, predict the action the model would take for this observation
with torch.no_grad():
    predicted_action_logits = wrapped_policy_network(sample_obs_tensor)
    predicted_action = torch.argmax(predicted_action_logits, dim=1).item()

print(f"\nSample Observation: {sample_obs_np}")
print(f"Predicted Action for this observation: {predicted_action}")

# Plot attributions for the predicted action
attributions_for_predicted_action = attributions_per_action[predicted_action]

plt.figure(figsize=(12, 6))
plt.barh(feature_names, attributions_for_predicted_action, color='skyblue')
plt.xlabel('LRP Attribution (Relevance)')
plt.ylabel('Features')
plt.title(f'LRP Attributions for Predicted Action ({predicted_action})')
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

print("LRP attributions computed and visualized for the predicted action.")

"""**Reasoning**:
The previous code block successfully implemented Layer-wise Relevance Propagation (LRP) and visualized the attributions, completing the current subtask. The next step is to summarize this implementation and its contribution to the overall goal.


"""

pip install torchviz

"""**Reasoning**:
Now that `torchviz` is installed, I will proceed with generating the computational graph visualization. This involves reloading the DataFrame, instantiating the environment, loading the trained model, extracting and wrapping the policy network, creating a dummy observation, performing a forward pass, and then using `torchviz.make_dot()` to visualize the graph.


"""

import pandas as pd
import torch
from stable_baselines3 import PPO
from model import SatelliteManeuverEnv # Import the environment class
from torchviz import make_dot # Import make_dot for visualization

# Re-load the DataFrame as it was not defined in the current scope
df = pd.read_csv('/content/CDM_DATA_YOTA.csv', low_memory=False)

# Instantiate the environment before loading the model
eval_env = SatelliteManeuverEnv(df) # Create a new env instance if needed

# Define custom_objects to handle deserialization issues, especially with numpy version changes
custom_objects = {
    "observation_space": eval_env.observation_space,
    "action_space": eval_env.action_space,
    "_last_obs": None, # Set to None to prevent deserialization issues if not critical
    "_last_episode_starts": None, # Set to None to prevent deserialization issues if not critical
}

# Load the trained model (from previous successful steps)
# Pass the instantiated environment and custom_objects to PPO.load()
loaded_model = PPO.load("ppo_satellite_maneuver.zip", env=eval_env, custom_objects=custom_objects)

# The PPO model's policy itself is the network that handles observations
# `loaded_model.policy` is an instance of ActorCriticPolicy.
policy_network = loaded_model.policy

# Ensure the policy network is in evaluation mode
policy_network.eval()

# Define a custom forward function for Captum to interpret
# This wrapper is used to ensure the forward pass is compatible with graph visualization tools.
class PolicyOutputWrapper(torch.nn.Module):
    def __init__(self, ppo_policy_module):
        super().__init__()
        # Our observations are already 1D and flat (shape 10,). The features_extractor for MlpPolicy
        # in SB3 typically contains a Flatten layer which can be problematic for LRP/graph viz.
        # Since the observation is already flat, we can directly use mlp_extractor and action_net.
        self.mlp_extractor = ppo_policy_module.mlp_extractor
        self.action_net = ppo_policy_module.action_net

    def forward(self, observation):
        # Ensure the observation is a batch, even if it's a single sample, as SB3 expects batched inputs
        if observation.dim() == 1:
            observation = observation.unsqueeze(0)

        # Pass the observation (already treated as flat features) directly to the mlp_extractor
        latent_pi, _ = self.mlp_extractor(observation)

        # Get logits from the action network
        action_logits = self.action_net(latent_pi)
        return action_logits

# Create an instance of the wrapper, passing the entire loaded_model.policy
wrapped_policy_network = PolicyOutputWrapper(policy_network)

# Get a sample observation from the environment
sample_obs_np, _ = eval_env.reset()
sample_obs_tensor = torch.tensor(sample_obs_np, dtype=torch.float32).unsqueeze(0) # Add batch dimension

# Perform a forward pass to generate the graph
output = wrapped_policy_network(sample_obs_tensor)

# Create and render the graph visualization
dot = make_dot(output, params=dict(wrapped_policy_network.named_parameters()))
dot.render("ppo_policy_graph", format="png", cleanup=True) # Saves as ppo_policy_graph.png

print("Computational graph visualization saved to ppo_policy_graph.png")

"""## Final Task

### Subtask:
Summarize the contents and purpose of the created `ppo_satellite_maneuver.zip` (the model file) and `agent_modules.json` (the configuration file), explaining their role in the overall agent setup for satellite maneuver optimization.

## Summary:

### Data Analysis Key Findings

*   **`ppo_satellite_maneuver.zip`**: This file represents the trained reinforcement learning model, specifically utilizing the Proximal Policy Optimization (PPO) algorithm. It encapsulates the learned policy (the "brain" of the agent) that determines optimal actions for satellite maneuvers based on observed states. The `.zip` format indicates it's a compressed archive likely containing the neural network weights, architecture definition, and other necessary data to restore and use the trained PPO agent for decision-making.
*   **`agent_modules.json`**: This file serves as the configuration file for the overall agent setup. It defines how the various components of the agent, including the trained model from `ppo_satellite_maneuver.zip`, are structured and interact. It typically specifies parameters for the reinforcement learning environment, observation and action spaces, and how the model should be loaded and integrated into the system for satellite maneuver optimization.

### Insights or Next Steps

*   These two files are fundamental for deploying and executing the trained satellite maneuver optimization agent. `ppo_satellite_maneuver.zip` provides the intelligence, and `agent_modules.json` provides the operational blueprint.
*   To further validate or utilize this setup, the next step would involve loading these files into a simulation or real-world environment to observe the agent's performance in optimizing satellite maneuvers according to its trained policy.
"""